{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error, r2_score\n",
    "import statistics\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Adamax\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../df.csv\")\n",
    "doc2vec50 = pd.read_csv(\"../dataset/doc2vec_50.csv\")\n",
    "df_train= df[[\"essay_set\", \"essay\",\"total_score\", \"word_count\",\"Mistakes\",\"reading_ease\"]]\n",
    "df_train = doc2vec50\n",
    "df_train = pd.concat([df_train, df[[\"essay_set\", \"word_count\",\"Mistakes\",\"reading_ease\"]]], axis = 1, join = \"inner\").drop(['Unnamed: 0'], axis = 1)\n",
    "y = df[\"total_score\"]\n",
    "X = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../df.csv\")\n",
    "doc2vec300 = pd.read_csv(\"../dataset/doc2vec_300.csv\")\n",
    "df_train= df[[\"essay_set\", \"essay\",\"total_score\", \"word_count\",\"Mistakes\",\"reading_ease\"]]\n",
    "df_train = doc2vec300\n",
    "df_train = pd.concat([df_train, df[[\"essay_set\", \"word_count\",\"Mistakes\",\"reading_ease\"]]], axis = 1, join = \"inner\").drop(['Unnamed: 0'], axis = 1)\n",
    "y = df[\"total_score\"].values\n",
    "X = df_train.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(54, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 54], return_sequences=True))\n",
    "    model.add(LSTM(54, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits = 5, shuffle = True)\n",
    "results = []\n",
    "r2s = []\n",
    "mses = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "for num in range(1,9):\n",
    "    essay_set = pd.read_csv(\"../dataset/cleandata_set_{}.csv\".format(num))\n",
    "    train_corpus = essay_set['essay']\n",
    "    test_corpus = train_corpus.copy()\n",
    "    vocab = CountVectorizer(stop_words='english', lowercase= True).fit(train_corpus)\n",
    "\n",
    "    # generate counts for a new set of documents\n",
    "    doc_emb = vocab.transform(train_corpus)\n",
    "    \n",
    "    vec_size = 50\n",
    "    window=2\n",
    "    min_count=1\n",
    "    workers=8\n",
    "    epochs=40\n",
    "    essays = [TaggedDocument(gensim.utils.simple_preprocess(doc), [i]) for i, doc in enumerate(train_corpus)]\n",
    "    \n",
    "    model = Doc2Vec(essays, vector_size=vec_size, window=window, min_count=min_count, workers=workers, epochs=epochs)\n",
    "    #might not need this line\n",
    "    model.train(essays, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    doc = [gensim.utils.simple_preprocess(doc) for i, doc in enumerate(test_corpus)]\n",
    "\n",
    "    doc_emb = np.zeros((len(doc), vec_size))\n",
    "    for i in range(len(doc)):\n",
    "        doc_emb[i,:] = model.infer_vector(doc[i])\n",
    "        \n",
    "    df_train = pd.DataFrame(doc_emb)\n",
    "    df_train = pd.concat([df_train, df[[\"essay_set\", \"word_count\",\"Mistakes\",\"reading_ease\"]]], axis = 1, join = \"inner\").drop(['Unnamed: 0'], axis = 1)\n",
    "    \n",
    "    y = df[\"total_score\"]\n",
    "    X = df_train\n",
    "    print(\"\\n###########Set-{}###########\\n\".format(num))\n",
    "    count = 1\n",
    "    result_for_set = []\n",
    "    for traincv, testcv in cv.split(X):\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "        \n",
    "        X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_test = X_test.to_numpy()\n",
    "\n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "        lstm_model = get_model()\n",
    "        lstm_model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
    "\n",
    "\n",
    "        y_pred_temp = lstm_model.predict(X_test)\n",
    "\n",
    "        y_pred = []\n",
    "        for i in y_pred_temp:\n",
    "            y_pred.append(int(round(i[0])))\n",
    "        y_pred_temp = np.around(y_pred_temp)\n",
    "        if count == 5:\n",
    "             lstm_model.save_weights('final_lstm.h5')\n",
    "        y_test_new = []\n",
    "        for  i in list(y_test.array):\n",
    "            y_test_new.append(int(i))\n",
    "        result = cohen_kappa_score(y_test_new,y_pred,weights='quadratic')\n",
    "        print(\"Kappa Score: {}\".format(result))\n",
    "        result_for_set.append(result)\n",
    "        count += 1\n",
    "    results.append(result_for_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay set 1\n",
      "0.7430904524409885\n",
      "Essay set 2\n",
      "0.6430423972736458\n",
      "Essay set 3\n",
      "0.608976790928228\n",
      "Essay set 4\n",
      "0.6923776715581466\n",
      "Essay set 5\n",
      "0.6636116129139091\n",
      "Essay set 6\n",
      "0.6727591464636252\n",
      "Essay set 7\n",
      "0.7504665038869414\n",
      "Essay set 8\n",
      "0.0019747832427356336\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,8):\n",
    "    print(\"Essay set {}\".format(i+1))\n",
    "    print(np.mean(results[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.53643310e-01,  1.22631383e+00,  1.49060738e+00, ...,\n",
       "         3.38000000e+02,  2.00000000e+01,  7.40200000e+01],\n",
       "       [ 1.78830183e+00, -5.99211633e-01, -2.10853004e+00, ...,\n",
       "         4.19000000e+02,  3.30000000e+01,  6.70800000e+01],\n",
       "       [-2.45267693e-02,  5.21169484e-01, -8.56822908e-01, ...,\n",
       "         2.79000000e+02,  2.00000000e+01,  6.82000000e+01],\n",
       "       ...,\n",
       "       [ 1.19136560e+00,  9.73212183e-01, -5.47442257e-01, ...,\n",
       "         8.18000000e+02,  8.20000000e+01,  6.92500000e+01],\n",
       "       [ 4.18220490e-01,  1.79089689e+00,  5.47983468e-01, ...,\n",
       "         5.94000000e+02,  3.30000000e+01,  7.20500000e+01],\n",
       "       [-8.03048015e-02,  1.26649606e+00,  1.87341690e-01, ...,\n",
       "         4.68000000e+02,  1.10000000e+01,  7.20500000e+01]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.loc[df[\"essay_set\"] == j][\"total_score\"]\n",
    "X = df_train.loc[df_train[\"essay_set\"] == j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        8.0\n",
       "1        9.0\n",
       "2        7.0\n",
       "3       10.0\n",
       "4        8.0\n",
       "        ... \n",
       "1778     8.0\n",
       "1779     7.0\n",
       "1780     8.0\n",
       "1781     2.0\n",
       "1782     7.0\n",
       "Name: total_score, Length: 1783, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Mistakes</th>\n",
       "      <th>reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.653643</td>\n",
       "      <td>1.226314</td>\n",
       "      <td>1.490607</td>\n",
       "      <td>-0.273509</td>\n",
       "      <td>-0.126667</td>\n",
       "      <td>1.948407</td>\n",
       "      <td>-0.800371</td>\n",
       "      <td>-0.520892</td>\n",
       "      <td>-1.989100</td>\n",
       "      <td>0.305569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.436761</td>\n",
       "      <td>1.510649</td>\n",
       "      <td>-0.300369</td>\n",
       "      <td>-0.430754</td>\n",
       "      <td>-1.164265</td>\n",
       "      <td>0.293608</td>\n",
       "      <td>1</td>\n",
       "      <td>338</td>\n",
       "      <td>20.0</td>\n",
       "      <td>74.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.788302</td>\n",
       "      <td>-0.599212</td>\n",
       "      <td>-2.108530</td>\n",
       "      <td>0.105541</td>\n",
       "      <td>-0.419768</td>\n",
       "      <td>0.424503</td>\n",
       "      <td>0.306117</td>\n",
       "      <td>-1.297395</td>\n",
       "      <td>-0.365960</td>\n",
       "      <td>-0.193469</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.120824</td>\n",
       "      <td>0.798896</td>\n",
       "      <td>-0.735984</td>\n",
       "      <td>-1.692172</td>\n",
       "      <td>-0.989390</td>\n",
       "      <td>1.180730</td>\n",
       "      <td>1</td>\n",
       "      <td>419</td>\n",
       "      <td>33.0</td>\n",
       "      <td>67.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.024527</td>\n",
       "      <td>0.521169</td>\n",
       "      <td>-0.856823</td>\n",
       "      <td>-0.467126</td>\n",
       "      <td>-0.856623</td>\n",
       "      <td>0.542416</td>\n",
       "      <td>-1.578355</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-1.748438</td>\n",
       "      <td>-0.108801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789562</td>\n",
       "      <td>0.019656</td>\n",
       "      <td>-1.198619</td>\n",
       "      <td>1.141882</td>\n",
       "      <td>-0.438716</td>\n",
       "      <td>-0.378533</td>\n",
       "      <td>1</td>\n",
       "      <td>279</td>\n",
       "      <td>20.0</td>\n",
       "      <td>68.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.232087</td>\n",
       "      <td>0.245010</td>\n",
       "      <td>-0.081397</td>\n",
       "      <td>2.165611</td>\n",
       "      <td>-1.296768</td>\n",
       "      <td>0.386925</td>\n",
       "      <td>-1.609302</td>\n",
       "      <td>0.097405</td>\n",
       "      <td>-2.632485</td>\n",
       "      <td>0.009429</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900712</td>\n",
       "      <td>1.323254</td>\n",
       "      <td>-1.008070</td>\n",
       "      <td>-2.154635</td>\n",
       "      <td>1.147291</td>\n",
       "      <td>1.223645</td>\n",
       "      <td>1</td>\n",
       "      <td>524</td>\n",
       "      <td>68.0</td>\n",
       "      <td>53.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.532192</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>-1.994325</td>\n",
       "      <td>0.103210</td>\n",
       "      <td>-1.187608</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>0.119099</td>\n",
       "      <td>0.494659</td>\n",
       "      <td>-0.015966</td>\n",
       "      <td>1.025912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127415</td>\n",
       "      <td>-0.186732</td>\n",
       "      <td>-2.030141</td>\n",
       "      <td>-2.104558</td>\n",
       "      <td>0.112237</td>\n",
       "      <td>-0.022088</td>\n",
       "      <td>1</td>\n",
       "      <td>465</td>\n",
       "      <td>23.0</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0.076996</td>\n",
       "      <td>1.060306</td>\n",
       "      <td>-0.374213</td>\n",
       "      <td>-0.734702</td>\n",
       "      <td>0.987911</td>\n",
       "      <td>3.004169</td>\n",
       "      <td>1.927672</td>\n",
       "      <td>0.183494</td>\n",
       "      <td>-2.541197</td>\n",
       "      <td>0.138758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343130</td>\n",
       "      <td>0.955492</td>\n",
       "      <td>0.042598</td>\n",
       "      <td>-0.262129</td>\n",
       "      <td>-1.845663</td>\n",
       "      <td>-2.069562</td>\n",
       "      <td>1</td>\n",
       "      <td>497</td>\n",
       "      <td>46.0</td>\n",
       "      <td>77.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0.526733</td>\n",
       "      <td>2.083623</td>\n",
       "      <td>-1.318357</td>\n",
       "      <td>0.365669</td>\n",
       "      <td>0.885012</td>\n",
       "      <td>1.430439</td>\n",
       "      <td>-0.444463</td>\n",
       "      <td>0.419993</td>\n",
       "      <td>-1.443482</td>\n",
       "      <td>0.104980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.393086</td>\n",
       "      <td>1.244183</td>\n",
       "      <td>-1.869425</td>\n",
       "      <td>-1.892759</td>\n",
       "      <td>-1.115894</td>\n",
       "      <td>-0.072377</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>17.0</td>\n",
       "      <td>84.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>0.258367</td>\n",
       "      <td>0.962929</td>\n",
       "      <td>1.196177</td>\n",
       "      <td>-0.475155</td>\n",
       "      <td>-1.180123</td>\n",
       "      <td>2.435375</td>\n",
       "      <td>-0.833579</td>\n",
       "      <td>0.755691</td>\n",
       "      <td>0.030383</td>\n",
       "      <td>1.244768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655146</td>\n",
       "      <td>-0.589399</td>\n",
       "      <td>-1.204976</td>\n",
       "      <td>1.377089</td>\n",
       "      <td>0.634938</td>\n",
       "      <td>-0.676763</td>\n",
       "      <td>1</td>\n",
       "      <td>291</td>\n",
       "      <td>14.0</td>\n",
       "      <td>71.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0.092214</td>\n",
       "      <td>0.710328</td>\n",
       "      <td>0.870210</td>\n",
       "      <td>-0.486119</td>\n",
       "      <td>-0.559627</td>\n",
       "      <td>0.609152</td>\n",
       "      <td>0.108019</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>-0.310847</td>\n",
       "      <td>0.426044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.377715</td>\n",
       "      <td>0.458104</td>\n",
       "      <td>-0.948134</td>\n",
       "      <td>-0.174676</td>\n",
       "      <td>-0.203892</td>\n",
       "      <td>-0.439191</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>-0.372615</td>\n",
       "      <td>0.728097</td>\n",
       "      <td>-0.096739</td>\n",
       "      <td>0.141957</td>\n",
       "      <td>-0.251851</td>\n",
       "      <td>-0.035748</td>\n",
       "      <td>-1.283910</td>\n",
       "      <td>1.236656</td>\n",
       "      <td>-1.723650</td>\n",
       "      <td>0.964451</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117982</td>\n",
       "      <td>0.623014</td>\n",
       "      <td>-1.433479</td>\n",
       "      <td>-1.176527</td>\n",
       "      <td>-0.433444</td>\n",
       "      <td>0.682913</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "      <td>20.0</td>\n",
       "      <td>76.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.653643  1.226314  1.490607 -0.273509 -0.126667  1.948407 -0.800371   \n",
       "1     1.788302 -0.599212 -2.108530  0.105541 -0.419768  0.424503  0.306117   \n",
       "2    -0.024527  0.521169 -0.856823 -0.467126 -0.856623  0.542416 -1.578355   \n",
       "3     2.232087  0.245010 -0.081397  2.165611 -1.296768  0.386925 -1.609302   \n",
       "4    -0.532192 -0.046631 -1.994325  0.103210 -1.187608  0.603498  0.119099   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1778  0.076996  1.060306 -0.374213 -0.734702  0.987911  3.004169  1.927672   \n",
       "1779  0.526733  2.083623 -1.318357  0.365669  0.885012  1.430439 -0.444463   \n",
       "1780  0.258367  0.962929  1.196177 -0.475155 -1.180123  2.435375 -0.833579   \n",
       "1781  0.092214  0.710328  0.870210 -0.486119 -0.559627  0.609152  0.108019   \n",
       "1782 -0.372615  0.728097 -0.096739  0.141957 -0.251851 -0.035748 -1.283910   \n",
       "\n",
       "             7         8         9  ...        44        45        46  \\\n",
       "0    -0.520892 -1.989100  0.305569  ...  1.436761  1.510649 -0.300369   \n",
       "1    -1.297395 -0.365960 -0.193469  ... -1.120824  0.798896 -0.735984   \n",
       "2    -0.025513 -1.748438 -0.108801  ... -0.789562  0.019656 -1.198619   \n",
       "3     0.097405 -2.632485  0.009429  ... -0.900712  1.323254 -1.008070   \n",
       "4     0.494659 -0.015966  1.025912  ...  0.127415 -0.186732 -2.030141   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1778  0.183494 -2.541197  0.138758  ...  0.343130  0.955492  0.042598   \n",
       "1779  0.419993 -1.443482  0.104980  ...  1.393086  1.244183 -1.869425   \n",
       "1780  0.755691  0.030383  1.244768  ... -0.655146 -0.589399 -1.204976   \n",
       "1781  0.114833 -0.310847  0.426044  ...  0.377715  0.458104 -0.948134   \n",
       "1782  1.236656 -1.723650  0.964451  ...  1.117982  0.623014 -1.433479   \n",
       "\n",
       "            47        48        49  essay_set  word_count  Mistakes  \\\n",
       "0    -0.430754 -1.164265  0.293608          1         338      20.0   \n",
       "1    -1.692172 -0.989390  1.180730          1         419      33.0   \n",
       "2     1.141882 -0.438716 -0.378533          1         279      20.0   \n",
       "3    -2.154635  1.147291  1.223645          1         524      68.0   \n",
       "4    -2.104558  0.112237 -0.022088          1         465      23.0   \n",
       "...        ...       ...       ...        ...         ...       ...   \n",
       "1778 -0.262129 -1.845663 -2.069562          1         497      46.0   \n",
       "1779 -1.892759 -1.115894 -0.072377          1         200      17.0   \n",
       "1780  1.377089  0.634938 -0.676763          1         291      14.0   \n",
       "1781 -0.174676 -0.203892 -0.439191          1          15       0.0   \n",
       "1782 -1.176527 -0.433444  0.682913          1         214      20.0   \n",
       "\n",
       "      reading_ease  \n",
       "0            74.02  \n",
       "1            67.08  \n",
       "2            68.20  \n",
       "3            53.34  \n",
       "4            72.66  \n",
       "...            ...  \n",
       "1778         77.30  \n",
       "1779         84.88  \n",
       "1780         71.04  \n",
       "1781         90.09  \n",
       "1782         76.42  \n",
       "\n",
       "[1783 rows x 54 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
